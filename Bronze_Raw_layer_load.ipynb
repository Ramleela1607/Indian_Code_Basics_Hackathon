{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f3cfb66-6c36-4cc4-ae00-d997c6d12e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### BRONZE LAYER LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d945fb9-9826-4c88-b282-cdb5e7ca6a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "\n",
    "**Purpose:**  \n",
    "This notebook performs the **Bronze layer ingestion**, creating synthetic or raw farm datasets for soil, crop, market, pest, and rainfall. These tables contains data and serve as the foundation for the Silver and Gold layers, AI advisory, and downstream analytics.\n",
    "\n",
    "**Workflow:**  \n",
    "1. **Generate synthetic data** for:\n",
    "   - Soil: moisture, temperature, humidity, precipitation, city/state/country.\n",
    "   - Crop: crop health, NDVI, leaf moisture, growth stage, city/state/country.\n",
    "   - Market: crop prices, city/state/country.\n",
    "   - Pest: pest risk, local weather conditions.\n",
    "   - Rainfall: mm, type, anomaly, city/state/country.\n",
    "2. **Country and city-level adjustments** for price, reflecting realistic variations.\n",
    "3. **Write Bronze tables** to Delta Lake (overwrite mode).\n",
    "4. **Audit logging**:\n",
    "   - Records workflow job ID, task name, start/end timestamps, status (SUCCESS/FAILED), and messages in Delta audit table.\n",
    "\n",
    "**Notes:**  \n",
    "- Update `catalog_name`, `schema_name`, `workflow_job_id`, and `audit_schema_name` to match your environment.  \n",
    "- In production, this can be replaced with ingestion from real IoT devices, weather stations, and market feeds.  \n",
    "- These Bronze tables feed directly into the **Silver layer transformation** for cleaning and standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc4c816-7bfd-4031-8a6f-fbfc53f933ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ACIN DATA"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88da8750-3d48-45cd-b964-a8edfd426924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS databricks_free_edition;\n",
    "CREATE SCHEMA IF NOT EXISTS databricks_free_edition.databricks_bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS databricks_free_edition.audit_logs;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8772049-c31f-4018-9f0e-5bcbb7e3d93f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "catalog_name = \"databricks_free_edition\"\n",
    "schema_name = \"databricks_bronze\"\n",
    "audit_schema_name = \"audit_logs\"\n",
    "workflow_job_id = dbutils.widgets.get(\"workflow_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f509b897-44b6-4996-8ee5-fa4cb13284ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# \uD83E\uDDFE Audit Table Setup\n",
    "# -------------------------------\n",
    "def write_audit(workflow_job_id,task, status, start_ts, end_ts, message=\"\"):\n",
    "    rows = [(workflow_job_id,task, status, start_ts, end_ts, message)]\n",
    "    schema = \"workflow_job_id STRING,task STRING, status STRING, start_time TIMESTAMP, end_time TIMESTAMP, message STRING\"\n",
    "    spark.createDataFrame(rows, schema=schema).write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "        f\"{catalog_name}.{audit_schema_name}.error_reporting_audit\"\n",
    "    )\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {catalog_name}.{audit_schema_name}.pipeline_audit (\n",
    "        task STRING,\n",
    "        status STRING,\n",
    "        start_time TIMESTAMP,\n",
    "        end_time TIMESTAMP,\n",
    "        message STRING\n",
    "    ) USING delta\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------\n",
    "# \uD83C\uDF3E Bronze ETL Start\n",
    "# -------------------------------\n",
    "task_name = \"bronze_ingest\"\n",
    "start_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "try:\n",
    "    print(f\"\uD83D\uDE80 Starting {task_name} ...\")\n",
    "\n",
    "    n = 1825  # ~5 years of daily records\n",
    "    dates = pd.date_range(start=pd.Timestamp.now().normalize(), periods=n, freq='D')\n",
    "\n",
    "    countries = [\"India\", \"USA\", \"Brazil\", \"China\", \"Australia\", \"Nigeria\", \"France\", \"Argentina\"]\n",
    "    world_locations = {\n",
    "        \"India\": [\"Chennai\", \"Delhi\", \"Mumbai\", \"Kolkata\", \"Bangalore\", \"Hyderabad\"],\n",
    "        \"USA\": [\"California\", \"Texas\", \"Iowa\", \"Kansas\", \"Nebraska\", \"Florida\"],\n",
    "        \"Brazil\": [\"São Paulo\", \"Curitiba\", \"Brasília\", \"Salvador\", \"Rio de Janeiro\"],\n",
    "        \"China\": [\"Beijing\", \"Shanghai\", \"Guangzhou\", \"Chengdu\", \"Wuhan\"],\n",
    "        \"Australia\": [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\"],\n",
    "        \"Nigeria\": [\"Lagos\", \"Abuja\", \"Kano\", \"Ibadan\", \"Port Harcourt\"],\n",
    "        \"France\": [\"Paris\", \"Lyon\", \"Marseille\", \"Toulouse\", \"Bordeaux\"],\n",
    "        \"Argentina\": [\"Buenos Aires\", \"Rosario\", \"Córdoba\", \"Mendoza\", \"La Plata\"]\n",
    "    }\n",
    "\n",
    "    crops = [\n",
    "        \"Rice\", \"Wheat\", \"Maize\", \"Sugarcane\", \"Cotton\", \"Soybean\",\n",
    "        \"Barley\", \"Coffee\", \"Cocoa\", \"Palm Oil\", \"Sorghum\", \"Potato\", \"Banana\"\n",
    "    ]\n",
    "    base_crop_prices = {\n",
    "    \"Rice\": (1800, 2500),\n",
    "    \"Wheat\": (1600, 2200),\n",
    "    \"Maize\": (1200, 1800),\n",
    "    \"Sugarcane\": (300, 400),\n",
    "    \"Cotton\": (5500, 7500),\n",
    "    \"Soybean\": (3500, 4800),\n",
    "    \"Barley\": (1500, 2100),\n",
    "    \"Coffee\": (12000, 20000),\n",
    "    \"Cocoa\": (9000, 15000),\n",
    "    \"Palm Oil\": (8000, 12000),\n",
    "    \"Sorghum\": (1400, 2000),\n",
    "    \"Potato\": (800, 1300),\n",
    "    \"Banana\": (600, 900)\n",
    "    }\n",
    "\n",
    "    # Country-level price multipliers (reflecting local economics)\n",
    "    country_price_factor = {\n",
    "        \"India\": 1.0,\n",
    "        \"USA\": 1.5,\n",
    "        \"Brazil\": 1.2,\n",
    "        \"China\": 1.1,\n",
    "        \"Australia\": 1.4,\n",
    "        \"Nigeria\": 0.8,\n",
    "        \"France\": 1.6,\n",
    "        \"Argentina\": 1.1\n",
    "    }\n",
    "\n",
    "    def city_variance(city):\n",
    "        return 1 + np.random.uniform(-0.1, 0.1)\n",
    "    \n",
    "    crop_names = np.random.choice(crops, n)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    chosen_countries = np.random.choice(countries, n)\n",
    "    chosen_cities = [np.random.choice(world_locations[c]) for c in chosen_countries]\n",
    "    chosen_states = [f\"Region-{i%5}\" for i in range(n)]\n",
    "\n",
    "    # -------------------------------\n",
    "    # \uD83C\uDF31 Generate Synthetic Tables\n",
    "    # -------------------------------\n",
    "\n",
    "    soil_df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"soil_moisture\": np.round(np.random.uniform(10, 80, n), 2),\n",
    "        \"temperature\": np.round(np.random.uniform(10, 45, n), 1),\n",
    "        \"humidity\": np.round(np.random.uniform(20, 95, n), 1),\n",
    "        \"precipitation_intensity\": np.round(np.random.uniform(0, 20, n), 2),\n",
    "        \"country\": chosen_countries,\n",
    "        \"state_or_region\": chosen_states,\n",
    "        \"city\": chosen_cities\n",
    "    })\n",
    "    \n",
    "    crop_df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"crop_name\": crop_names,\n",
    "    \"crop_health_score\": np.random.randint(50, 100, n),\n",
    "    \"ndvi_index\": np.round(np.random.uniform(0.3, 0.9, n), 3),\n",
    "    \"leaf_moisture\": np.round(np.random.uniform(25, 90, n), 1),\n",
    "    \"growth_stage\": np.random.choice([\"Germination\",\"Vegetative\",\"Flowering\",\"Maturity\"], n),\n",
    "    \"country\": chosen_countries,\n",
    "    \"state_or_region\": chosen_states,\n",
    "    \"city\": chosen_cities\n",
    "    })\n",
    "\n",
    "    # Generate price per quintal based on crop, country, and city\n",
    "    price_per_quintal = []\n",
    "    for i in range(n):\n",
    "        crop = crop_df.loc[i, \"crop_name\"]\n",
    "        country = crop_df.loc[i, \"country\"]\n",
    "        city = crop_df.loc[i, \"city\"]\n",
    "\n",
    "        base_low, base_high = base_crop_prices[crop]\n",
    "        base_price = np.random.uniform(base_low, base_high)\n",
    "        adjusted_price = base_price * country_price_factor[country] * city_variance(city)\n",
    "\n",
    "        price_per_quintal.append(round(adjusted_price, 2))\n",
    "\n",
    "    crop_df[\"crop_price_per_quintal\"] = price_per_quintal\n",
    "\n",
    "    market_df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"crop_name\": np.random.choice(crops, n),\n",
    "        \"crop_price\": np.round(80 + np.cumsum(np.random.normal(0, 1, n)), 2),\n",
    "        \"country\": chosen_countries,\n",
    "        \"state_or_region\": chosen_states,\n",
    "        \"city\": chosen_cities\n",
    "    })\n",
    "\n",
    "    pest_df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"pest_risk\": np.random.randint(0, 100, n),\n",
    "        \"temperature\": np.round(np.random.uniform(15, 40, n), 1),\n",
    "        \"humidity\": np.round(np.random.uniform(30, 90, n), 1),\n",
    "        \"precipitation_intensity\": np.round(np.random.uniform(0, 10, n), 2),\n",
    "        \"country\": chosen_countries,\n",
    "        \"state_or_region\": chosen_states,\n",
    "        \"city\": chosen_cities\n",
    "    })\n",
    "\n",
    "    rainfall_df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"rainfall_mm\": np.round(np.random.uniform(0, 250, n), 1),\n",
    "        \"rainfall_type\": np.random.choice([\"Drizzle\", \"Light Rain\", \"Heavy Rain\", \"Storm\", \"None\"], n),\n",
    "        \"rainfall_anomaly\": np.round(np.random.uniform(-10, 10, n), 2),\n",
    "        \"country\": chosen_countries,\n",
    "        \"state_or_region\": chosen_states,\n",
    "        \"city\": chosen_cities\n",
    "    })\n",
    "\n",
    "    # -------------------------------\n",
    "    # \uD83D\uDCBE Write Bronze Tables\n",
    "    # -------------------------------\n",
    "    spark.createDataFrame(soil_df).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bronze_soil\")\n",
    "    spark.createDataFrame(crop_df).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bronze_crop\")\n",
    "    spark.createDataFrame(market_df).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bronze_market\")\n",
    "    spark.createDataFrame(pest_df).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bronze_pest\")\n",
    "    spark.createDataFrame(rainfall_df).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.bronze_rainfall\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # \uD83E\uDDFE Audit Logging\n",
    "    # -------------------------------\n",
    "    end_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    write_audit(workflow_job_id,task_name, \"SUCCESS\", start_ts, end_ts, \"Bronze tables created successfully \uD83C\uDF0E\")\n",
    "\n",
    "    print(\"✅ Bronze Ingest Completed Successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    end_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    tb = traceback.format_exc()\n",
    "    write_audit(workflow_job_id,task_name, \"FAILED\", start_ts, end_ts, str(tb)[:4000])\n",
    "    print(\"❌ Bronze ingest failed. See audit table for details.\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6776144891252527,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Raw_layer_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}