{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f656ed9-b6ec-41bd-9dba-56ae3e446d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SILVER LAYER LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "726eb167-712c-4c8c-942c-dbe129fec28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Purpose:**  \n",
    "This notebook performs the **Silver layer transformation**, converting raw Bronze-level farm datasets into standardized, clean, and ready-to-use Silver tables. These tables serve as the foundation for Gold layer aggregation, AI insights, and downstream analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6318becf-180a-4fd5-9002-9b0f7881a2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Workflow:**  \n",
    "1. **Load Bronze tables** (soil, crop, market, pest, rainfall) from Delta Lake.  \n",
    "2. **Standardize column names** to camelCase for consistency.  \n",
    "3. **Clean string fields** (trim whitespace, lowercase city and crop names).  \n",
    "4. **Standardize date columns** to proper date format.  \n",
    "5. **Unify city and crop names** across all tables for proper joins.  \n",
    "6. **Write cleaned Silver tables** to Delta Lake (overwrite mode).  \n",
    "7. **Audit logging**:  \n",
    "   - Record start/end timestamps, task name, status (SUCCESS/FAILED), and messages in a Delta audit table.\n",
    "\n",
    "**Notes:**  \n",
    "- Update `catalog_name`, `schema_name_bronze`, `schema_name`, `audit_schema_name`, and `workflow_job_id` to match your environment.  \n",
    "- If any Bronze table is missing, the notebook will log a FAILURE in the audit table.  \n",
    "- These Silver tables feed directly into **Gold layer aggregation** for advanced analytics and AI farm advisory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2478127f-8f5c-4bfd-942f-e7cc6edc5afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, lower, to_date, current_timestamp\n",
    "import traceback\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c285b1f-2d8b-4ac8-830d-fbf514fd8242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS databricks_free_edition.databricks_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d4f2457-0897-40c4-9616-2a7f35adac30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"databricks_free_edition\"\n",
    "schema_name = \"databricks_silver\"\n",
    "schema_name_bronze = \"databricks_bronze\"\n",
    "audit_schema_name = \"audit_logs\"\n",
    "workflow_job_id = dbutils.widgets.get(\"workflow_job_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147663db-06af-4e54-8131-42964e2ab5b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_audit(workflow_job_id,task, status, start_ts, end_ts, message=\"\"):\n",
    "    rows = [(workflow_job_id,task, status, start_ts, end_ts, message)]\n",
    "    schema = \"workflow_job_id STRING,task STRING, status STRING, start_time TIMESTAMP, end_time TIMESTAMP, message STRING\"\n",
    "    spark.createDataFrame(rows, schema=schema).write.format(\"delta\").mode(\"append\").saveAsTable(\n",
    "        f\"{catalog_name}.{audit_schema_name}.error_reporting_audit\"\n",
    "    )\n",
    "\n",
    "task_name = \"silver_transform\"\n",
    "start_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "\n",
    "try:\n",
    "    print(f\"Starting {task_name} ...\")\n",
    "\n",
    "    # Load Bronze (if missing, this will throw and be captured)\n",
    "    bronze_soil = spark.table(f\"{catalog_name}.{schema_name_bronze}.bronze_soil\")\n",
    "    bronze_crop = spark.table(f\"{catalog_name}.{schema_name_bronze}.bronze_crop\")\n",
    "    bronze_market = spark.table(f\"{catalog_name}.{schema_name_bronze}.bronze_market\")\n",
    "    bronze_pest = spark.table(f\"{catalog_name}.{schema_name_bronze}.bronze_pest\")\n",
    "    bronze_rainfall = spark.table(f\"{catalog_name}.{schema_name_bronze}.bronze_rainfall\")\n",
    "\n",
    "\n",
    "    # Helper: camelCase renamer\n",
    "    def to_camel_case(col_name):\n",
    "        parts = col_name.replace(\" \", \"_\").split(\"_\")\n",
    "        return parts[0].lower() + \"\".join(p.title() for p in parts[1:])\n",
    "\n",
    "    # Generic cleaning function\n",
    "    def clean_df(df):\n",
    "        # rename columns to camelCase\n",
    "        for c in df.columns:\n",
    "            df = df.withColumnRenamed(c, to_camel_case(c))\n",
    "        # trim string columns and lowercase city/crop names\n",
    "        string_cols = [f.name for f in df.schema.fields if \"string\" in f.dataType.simpleString()]\n",
    "        for sc in string_cols:\n",
    "            df = df.withColumn(sc, trim(col(sc)))\n",
    "        # standardize date column if present\n",
    "        if \"date\" in df.columns:\n",
    "            df = df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "        return df\n",
    "\n",
    "    soil_silver = clean_df(bronze_soil)\n",
    "    crop_silver = clean_df(bronze_crop)\n",
    "    market_silver = clean_df(bronze_market)\n",
    "    pest_silver = clean_df(bronze_pest)\n",
    "    bronze_rainfall = clean_df(bronze_rainfall)\n",
    "\n",
    "    # Additional standardizations: unify city/crop names lowercase for joins\n",
    "    def std_names(df):\n",
    "        if \"city\" in df.columns:\n",
    "            df = df.withColumn(\"city\", lower(col(\"city\")))\n",
    "        if \"cropName\" in df.columns:\n",
    "            df = df.withColumn(\"cropName\", lower(col(\"cropName\")))\n",
    "        return df\n",
    "\n",
    "    soil_silver = std_names(soil_silver)\n",
    "    crop_silver = std_names(crop_silver)\n",
    "    market_silver = std_names(market_silver)\n",
    "    pest_silver = std_names(pest_silver)\n",
    "    bronze_rainfall = std_names(bronze_rainfall)\n",
    "\n",
    "    # Write Silver tables (overwrite)\n",
    "    soil_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.silver_soil\")\n",
    "    crop_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.silver_crop\")\n",
    "    market_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.silver_market\")\n",
    "    pest_silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.silver_pest\")\n",
    "    bronze_rainfall.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.silver_rainfall\")\n",
    "\n",
    "\n",
    "    end_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    write_audit(workflow_job_id,task_name, \"SUCCESS\", start_ts, end_ts, \"Silver tables created\")\n",
    "    print(\"Silver transform completed successfully.\")\n",
    "except Exception as e:\n",
    "    end_ts = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    tb = traceback.format_exc()\n",
    "    write_audit(workflow_job_id,task_name, \"FAILED\", start_ts, end_ts, str(tb)[:4000])\n",
    "    print(\"Silver transform failed. See audit table for details.\")\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7187789842795806,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_clean_Layer_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}