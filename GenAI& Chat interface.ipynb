{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ee6c227-e6e4-4991-9356-ba438678111c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### VECTOR TABLE CREATION & GEN AI IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a51285af-c9dd-4b8b-9965-e6b4e6ac7b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook provides an end-to-end system for farm advisory using a combination of:\n",
    "\n",
    "- Gold-layer aggregated farm data (soil, crop, market, pest, rainfall).  \n",
    "- Vector-based semantic search for user queries.  \n",
    "- AI-generated recommendations using **FLAN-T5-Large**.  \n",
    "- Visualizations of top crops by yield, profitability, rainfall trends, and price heatmaps.  \n",
    "- Automatic audit logging of user queries and AI responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4590426-3063-4937-8285-52fcbf32ec38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Features:**  \n",
    "1. Query by city, crop, or relative time (e.g., \"next week\", \"in 3 days\").  \n",
    "2. Fuzzy matching for cities and crops to handle typos.  \n",
    "3. AI-generated, concise, actionable recommendations for farmers.  \n",
    "4. Vector similarity fallback when exact data is unavailable.  \n",
    "5. Interactive visualizations:\n",
    "   - Bar charts for top yield and profitability per crop.\n",
    "   - Line plots for rainfall trends.\n",
    "   - Heatmaps for monthly crop prices.\n",
    "6. Audit logs stored in Delta tables for monitoring user queries and system responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0793303f-2d6f-44cd-83d3-eb38bc55c525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Usage:**  \n",
    "- Run sequentially in a Databricks cluster with CPU/GPU memory sufficient for **FLAN-T5-Large**.  \n",
    "- Update the `catalog_name`, `schema_name_gold`, and `audit_schema_name` variables to match your workspace.  \n",
    "- Enter a query in the last cell (`user_query = \"...\"`) and run to get AI recommendations and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d964083-bcdb-4d42-b92f-93ef07efbc34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install fuzzywuzzy[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a5a4dd-38be-478a-9ffa-414b2633996e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7addc9-7a5e-454a-a9d9-de3cd069d672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"databricks_free_edition\"\n",
    "schema_name = \"databricks_gold\"\n",
    "schema_name_silver = \"databricks_silver\"\n",
    "audit_schema_name = \"audit_logs\"\n",
    "schema_name_gold = \"databricks_gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c46ddd-0442-4cad-8eb5-9253253f4265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Imports\n",
    "# -----------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd, numpy as np, re, time, contextlib, io, traceback\n",
    "from datetime import datetime, timedelta\n",
    "from difflib import get_close_matches\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG (set these before running)\n",
    "# -----------------------------\n",
    "\n",
    "vector_table = f\"{catalog_name}.{schema_name_gold}.farm_vector_index\"\n",
    "audit_table = f\"{catalog_name}.{audit_schema_name}.farm_chat_audit\"\n",
    "\n",
    "# -----------------------------\n",
    "# INIT SPARK\n",
    "# -----------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{audit_schema_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD VECTOR TABLE\n",
    "# -----------------------------\n",
    "pdf = spark.table(vector_table).toPandas()\n",
    "pdf[\"date\"] = pd.to_datetime(pdf[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------------\n",
    "# PREPARE EMBEDDINGS MATRIX (fast similarity)\n",
    "# -----------------------------\n",
    "emb_matrix = None\n",
    "if \"embedding\" in pdf.columns and len(pdf) > 0:\n",
    "    try:\n",
    "        emb_matrix = np.stack(pdf[\"embedding\"].apply(lambda x: np.array(x, dtype=np.float32)).values)\n",
    "    except Exception:\n",
    "        # attempt eval parse if stored as string\n",
    "        emb_matrix = np.stack(pdf[\"embedding\"].apply(lambda x: np.array(eval(x), dtype=np.float32)).values)\n",
    "# If embedding column missing, emb_matrix stays None and fallback will be limited.\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD MODELS\n",
    "# -----------------------------\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FLAN-T5-LARGE for stable paragraph generation (Option A)\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_tokens=300):\n",
    "    \"\"\"Deterministic generation with repetition penalty to avoid loops.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = gen_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=1.2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS: fuzzy, date, vector query\n",
    "# -----------------------------\n",
    "def resolve_fuzzy_match(query, choices, cutoff=0.6):\n",
    "    if not choices:\n",
    "        return None\n",
    "    matches = get_close_matches(query.lower(), [c.lower() for c in choices], n=1, cutoff=cutoff)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def resolve_city_strong(query):\n",
    "    \"\"\"Try substring detection (fast), then fuzzy fallback, return original-cased city.\"\"\"\n",
    "    q = (query or \"\").lower()\n",
    "    cities = pdf[\"city\"].dropna().unique().tolist()\n",
    "    for c in cities:\n",
    "        if c.lower() in q:\n",
    "            return c\n",
    "    matched_lower = resolve_fuzzy_match(query, cities, cutoff=0.5)\n",
    "    if matched_lower:\n",
    "        for orig in cities:\n",
    "            if orig.lower() == matched_lower:\n",
    "                return orig\n",
    "    return None\n",
    "\n",
    "def extract_relative_date(query):\n",
    "    q = (query or \"\").lower()\n",
    "    today = datetime.now().date()\n",
    "    if \"tomorrow\" in q:\n",
    "        return today + timedelta(days=1)\n",
    "    if \"next week\" in q:\n",
    "        return today + timedelta(days=7)\n",
    "    m = re.search(r\"in (\\d+) days?\", q)\n",
    "    if m:\n",
    "        return today + timedelta(days=int(m.group(1)))\n",
    "    return None\n",
    "\n",
    "def query_vector_db_fast(user_query, top_k=5, df=pdf, emb_matrix_local=emb_matrix):\n",
    "    \"\"\"Vector similarity fallback using precomputed embeddings matrix for speed.\"\"\"\n",
    "    if emb_matrix_local is None or df.empty:\n",
    "        return df.head(0)\n",
    "    qv = embed_model.encode(user_query)\n",
    "    qv = np.array(qv, dtype=np.float32)\n",
    "    emb_norm = emb_matrix_local / np.linalg.norm(emb_matrix_local, axis=1, keepdims=True)\n",
    "    qv_norm = qv / np.linalg.norm(qv)\n",
    "    sims = emb_norm.dot(qv_norm)\n",
    "    top_idx = np.argsort(-sims)[:top_k]\n",
    "    return df.iloc[top_idx].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# AUDIT TABLE (init + append)\n",
    "# -----------------------------\n",
    "def init_audit_table():\n",
    "    if not spark.catalog.tableExists(audit_table):\n",
    "        schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"user_query\", StringType(), True),\n",
    "            StructField(\"user_location\", StringType(), True),\n",
    "            StructField(\"matched_city\", StringType(), True),\n",
    "            StructField(\"matched_crop\", StringType(), True),\n",
    "            StructField(\"answer\", StringType(), True),\n",
    "            StructField(\"response_time\", DoubleType(), True)\n",
    "        ])\n",
    "        spark.createDataFrame([], schema=schema).write.saveAsTable(audit_table)\n",
    "\n",
    "def append_audit_safe(entry: dict):\n",
    "    row = {\n",
    "        \"timestamp\": entry.get(\"timestamp\", datetime.now()),\n",
    "        \"user_query\": str(entry.get(\"user_query\") or \"\"),\n",
    "        \"user_location\": str(entry.get(\"user_location\") or \"\"),\n",
    "        \"matched_city\": str(entry.get(\"matched_city\") or \"\"),\n",
    "        \"matched_crop\": str(entry.get(\"matched_crop\") or \"\"),\n",
    "        \"answer\": str(entry.get(\"answer\") or \"\"),\n",
    "        \"response_time\": float(entry.get(\"response_time\") or 0.0)\n",
    "    }\n",
    "    sdf = spark.createDataFrame([row])\n",
    "    sdf.write.mode(\"append\").saveAsTable(audit_table)\n",
    "\n",
    "init_audit_table()\n",
    "\n",
    "# -----------------------------\n",
    "# SENTENCE CLEANER (robust)\n",
    "# -----------------------------\n",
    "def clean_and_build_paragraph(raw_text, min_words=30, max_sentences=5):\n",
    "    raw = (raw_text or \"\").strip()\n",
    "    # split by lines first (preserve multi-line)\n",
    "    lines = [l.strip() for l in raw.split(\"\\n\") if l.strip()]\n",
    "    sentence_candidates = []\n",
    "    for line in lines:\n",
    "        parts = re.split(r'(?<=[.!?])\\s+', line)\n",
    "        for p in parts:\n",
    "            if p.strip():\n",
    "                sentence_candidates.append(p.strip())\n",
    "    # dedupe\n",
    "    seen = set(); final = []\n",
    "    for s in sentence_candidates:\n",
    "        k = s.lower()\n",
    "        if k not in seen:\n",
    "            final.append(s)\n",
    "            seen.add(k)\n",
    "        if len(final) >= max_sentences:\n",
    "            break\n",
    "    # fallback to raw if too short\n",
    "    answer = \" \".join(final)\n",
    "    if len(answer.split()) < min_words:\n",
    "        # try taking raw but truncating to reasonable length\n",
    "        if len(raw.split()) > min_words:\n",
    "            answer = \" \".join(raw.split()[:max( min_words, 60 )])  # provide something substantive\n",
    "        else:\n",
    "            answer = raw\n",
    "    return answer\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN: ai_response (final corrected)\n",
    "# -----------------------------\n",
    "def ai_response(user_query, user_location=None, top_k=5):\n",
    "    start_t = time.time()\n",
    "\n",
    "    # detect city & crop\n",
    "    matched_city = resolve_city_strong(user_query)\n",
    "    crops = pdf[\"market_cropName\"].dropna().unique().tolist()\n",
    "    matched_crop = None\n",
    "    for c in crops:\n",
    "        if c.lower() in (user_query or \"\").lower():\n",
    "            matched_crop = c\n",
    "            break\n",
    "    if not matched_crop:\n",
    "        matched_crop = resolve_fuzzy_match(user_query, crops, cutoff=0.5)\n",
    "\n",
    "    # initial filter\n",
    "    df = pdf.copy()\n",
    "    if matched_city:\n",
    "        df = df[df[\"city\"].str.lower() == matched_city.lower()]\n",
    "    if matched_crop:\n",
    "        df = df[df[\"market_cropName\"].str.lower() == matched_crop.lower()]\n",
    "\n",
    "    # relative date with +/- window\n",
    "    qd = extract_relative_date(user_query)\n",
    "    if qd is not None and not df.empty:\n",
    "        avail = df[\"date\"].dropna().sort_values()\n",
    "        if not avail.empty:\n",
    "            closest = min(avail, key=lambda x: abs(pd.Timestamp(x).date() - qd))\n",
    "            window = timedelta(days=15)\n",
    "            df = df[(df[\"date\"] >= (closest - window)) & (df[\"date\"] <= (closest + window))]\n",
    "\n",
    "    # vector fallback preferring matched city\n",
    "    if df.empty:\n",
    "        vec = query_vector_db_fast(user_query, top_k=50, df=pdf, emb_matrix_local=emb_matrix)\n",
    "        if matched_city and \"city\" in vec.columns:\n",
    "            vec_city = vec[vec[\"city\"].str.lower() == matched_city.lower()]\n",
    "            if not vec_city.empty:\n",
    "                df = vec_city.head(top_k)\n",
    "            else:\n",
    "                df = vec.head(top_k)\n",
    "        else:\n",
    "            df = vec.head(top_k)\n",
    "\n",
    "    # final enforce city filter for plotting clarity\n",
    "    if matched_city and \"city\" in df.columns:\n",
    "        df = df[df[\"city\"].str.lower() == matched_city.lower()]\n",
    "\n",
    "    top = df.sort_values(\"yieldPredictionScore\", ascending=False).head(top_k)\n",
    "\n",
    "    # build summary and prompt\n",
    "    if top.empty:\n",
    "        answer = \"I couldn't find relevant data for that location or crop. Try a different city or crop name.\"\n",
    "    else:\n",
    "        grp = top.groupby(\"market_cropName\").agg({\n",
    "            \"yieldPredictionScore\": \"mean\",\n",
    "            \"profitabilityIndex\": \"mean\",\n",
    "            \"rainfall_rainfallMm\": \"mean\"\n",
    "        }).reset_index()\n",
    "\n",
    "        best_crop = grp.loc[grp[\"profitabilityIndex\"].idxmax(), \"market_cropName\"]\n",
    "        city_text = matched_city or (user_location or \"your area\")\n",
    "\n",
    "        avg_yield = float(grp[\"yieldPredictionScore\"].mean())\n",
    "        avg_profit = float(grp[\"profitabilityIndex\"].mean())\n",
    "        avg_rain = float(grp[\"rainfall_rainfallMm\"].mean())\n",
    "\n",
    "        yield_trend = \"increasing\" if avg_yield > 30 else \"stable\" if avg_yield > 20 else \"declining\"\n",
    "        rain_comment = (\"favorable for most field crops\" if avg_rain > 20 else\n",
    "                        \"slightly below ideal levels\" if avg_rain > 5 else\n",
    "                        \"very low; irrigation may be needed\")\n",
    "\n",
    "        summary = (\n",
    "            f\"In {city_text}, {best_crop} looks most promising. Average yield ≈ {avg_yield:.2f}, \"\n",
    "            f\"profitability ≈ {avg_profit:.2f}, and recent rainfall ≈ {avg_rain:.1f} mm ({rain_comment}). \"\n",
    "            f\"Yield trend appears {yield_trend}. Recommend timely sowing, balanced nutrition, and pest monitoring.\"\n",
    "        )\n",
    "\n",
    "        context_lines = []\n",
    "        for _, r in top.iterrows():\n",
    "            context_lines.append(\n",
    "                f\"{r['market_cropName']} (city:{r['city']}): Yield={r['yieldPredictionScore']:.2f}, \"\n",
    "                f\"Profit={r['profitabilityIndex']:.2f}, Rain={r.get('rainfall_rainfallMm',0):.1f}mm\"\n",
    "            )\n",
    "        context = \"\\n\".join(context_lines)\n",
    "\n",
    "        prompt = (\n",
    "            \"You are an expert agricultural advisor. Using the numeric summary and context below, \"\n",
    "            \"write a clear 4-sentence recommendation paragraph for farmers in simple language. \"\n",
    "            f\"Start by naming the best crop for {city_text}, explain why (mention yield, profit, rainfall), \"\n",
    "            \"and end with one practical risk or alternate option.\\n\\n\"\n",
    "            f\"Summary:\\n{summary}\\n\\nContext:\\n{context}\\n\\nQuestion: {user_query}\"\n",
    "        )\n",
    "\n",
    "        raw = generate_text(prompt, max_tokens=260)\n",
    "        # clean and build paragraph robustly\n",
    "        answer = clean_and_build_paragraph(raw, min_words=30, max_sentences=5)\n",
    "\n",
    "    # audit\n",
    "    rt = round(time.time() - start_t, 2)\n",
    "    append_audit_safe({\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"user_query\": user_query,\n",
    "        \"user_location\": user_location,\n",
    "        \"matched_city\": matched_city,\n",
    "        \"matched_crop\": matched_crop,\n",
    "        \"answer\": answer,\n",
    "        \"response_time\": rt\n",
    "    })\n",
    "\n",
    "    print(answer)\n",
    "    return answer, top\n",
    "\n",
    "# -----------------------------\n",
    "# PLOTTING HELPERS\n",
    "# -----------------------------\n",
    "def plot_top_matches(top_matches):\n",
    "    if top_matches is None or top_matches.empty:\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    cities = top_matches[\"city\"].dropna().unique()\n",
    "    if len(cities) > 1:\n",
    "        main_city = top_matches[\"city\"].mode().iloc[0]\n",
    "        top_matches = top_matches[top_matches[\"city\"] == main_city]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=\"market_cropName\", y=\"yieldPredictionScore\", data=top_matches)\n",
    "    plt.title(f\"Yield Score — {top_matches['city'].iloc[0]}\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    display(plt.gcf())\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=\"market_cropName\", y=\"profitabilityIndex\", data=top_matches)\n",
    "    plt.title(f\"Profitability — {top_matches['city'].iloc[0]}\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    display(plt.gcf())\n",
    "\n",
    "    if \"date\" in top_matches.columns and top_matches[\"date\"].notna().any():\n",
    "        plt.figure(figsize=(12,5))\n",
    "        sns.lineplot(x=\"date\", y=\"rainfall_rainfallMm\", hue=\"market_cropName\", data=top_matches)\n",
    "        plt.title(f\"Rainfall Trend — {top_matches['city'].iloc[0]}\")\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.tight_layout()\n",
    "        display(plt.gcf())\n",
    "\n",
    "def plot_price_heatmap(df):\n",
    "    if df.empty:\n",
    "        print(\"No data for heatmap.\")\n",
    "        return\n",
    "    d = df.copy()\n",
    "    d[\"date\"] = pd.to_datetime(d[\"date\"])\n",
    "    d[\"month\"] = d[\"date\"].dt.strftime(\"%b\")\n",
    "    heat = (d.groupby([\"market_cropName\",\"month\"])[\"crop_cropPricePerQuintal\"]\n",
    "            .mean().reset_index().pivot(index=\"market_cropName\", columns=\"month\", values=\"crop_cropPricePerQuintal\"))\n",
    "    month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "    cols = [m for m in month_order if m in heat.columns]\n",
    "    heat = heat.reindex(columns=cols)\n",
    "    plt.figure(figsize=(14,8))\n",
    "    sns.heatmap(heat, annot=True, fmt=\".1f\", linewidths=.5, cmap=\"YlGnBu\")\n",
    "    plt.title(\"Average Crop Price per Quintal — By Month\")\n",
    "    plt.tight_layout()\n",
    "    display(plt.gcf())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d051ef5-5ed6-4f2d-8b31-5f2540ef6bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_query = \"wich crop shuld i grow in California next week\"\n",
    "answer, top = ai_response(user_query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GenAI& Chat interface",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}